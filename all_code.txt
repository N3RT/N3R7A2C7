# SNAPSHOT OF PROJECT FILES (TEXT)
# Each file is separated by a marker line: ## FILE: <relative_path>

## FILE: DEV_LOG.txt
[2026-02-05 11:59] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Ä–∫–∞—Å–∞ –ø—Ä–æ–µ–∫—Ç–∞ rag_advct –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ ollama_chat_4b.

1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
- –°–æ–∑–¥–∞–Ω –±–∞–∑–æ–≤—ã–π –∫–∞—Ä–∫–∞—Å –∫–∞—Ç–∞–ª–æ–≥–∞ rag_advct –ø–æ –¢–ó RAG-—Å–∏—Å—Ç–µ–º—ã:
  - app/ (–∏—Å—Ö–æ–¥–Ω–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞)
  - app/api/, app/core/, app/ingestion/, app/postprocessing/, app/bot/, app/cli/
  - config/ (system.yaml + –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è tasks/)
  - data/ (data/.gitkeep, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–¥ chromadb/sqlite/logs)
  - docker/ (Dockerfile, docker-compose.dev.yml ‚Äì –ø–æ–∫–∞ –ø—É—Å—Ç—ã–µ –∑–∞–≥–ª—É—à–∫–∏)
  - llm_connectors/ (–º–æ–¥—É–ª–∏ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LLM)
  - tests/ (test_api.py ‚Äì –∑–∞–≥–æ—Ç–æ–≤–∫–∞)
- –í .gitignore –¥–æ–±–∞–≤–ª–µ–Ω—ã:
  - .env
  - –∫–∞—Ç–∞–ª–æ–≥ data/ (–∫—Ä–æ–º–µ data/.gitkeep)
  - *.log, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ *.md.tmp, *.docx
  - all_code.py, DEV_LOG.txt, PROJECT_PLAN.txt
  —á—Ç–æ–±—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ –±—ã–ª–æ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–æ–≤ –∏ —Å–Ω–∞–ø—à–æ—Ç–æ–≤.

2. –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –°–æ–∑–¥–∞–Ω–æ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ .venv –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞.
- –í .venv —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:
  - fastapi[standard]
  - uvicorn
  - requests
  - httpx
- –í—Å–µ –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∫–æ–º–∞–Ω–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —á–µ—Ä–µ–∑:
  - source .venv/bin/activate

3. –°–∏—Å—Ç–µ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª config/system.yaml —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:
  - environment: dev
  - paths:
      data_root: ./data
      chromadb_dir: ./data/chromadb
      sqlite_path: ./data/sqlite/tables.db
      logs_path: ./data/logs/app.log
  - llm:
      mode: dev
      connector: "llm_connectors.connector_dev:call_llm"
- –≠—Ç–æ –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥-–ª–æ–∞–¥–µ—Ä–∞ –∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ (dev/test/prod) —Å—Ç—Ä–æ–≥–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥, –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ –¢–ó.

4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ ollama_chat_4b
- –ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö API (DeepSeek/OpenRouter) –≤ dev –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ –∏ –∑–∞–≤—è–∑–∫–∏ –Ω–∞ –±–∏–ª–ª–∏–Ω–≥.
- –í –∫–∞—á–µ—Å—Ç–≤–µ dev-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–π —Å—Ç–µ–∫:
  - –¥–≤–∏–∂–æ–∫ Ollama —Å –º–æ–¥–µ–ª—è–º–∏ llama3.2:1b –∏ llama3.2:3b
  - –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å ollama_chat_4b (FastAPI + uvicorn) —Å HTTP API:
    - POST http://127.0.0.1:4004/api/chat
    - —Ç–µ–ª–æ: {"messages": [...], "model": "llama3.2:1b"|"llama3.2:3b" (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)}
    - –æ—Ç–≤–µ—Ç: {"reply": "...", "raw": {... –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç Ollama ...}}
- –í rag_advct —Å–æ–∑–¥–∞–Ω –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä llm_connectors/connector_dev.py:
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç httpx.AsyncClient —Å —Ç–∞–π–º–∞—É—Ç–æ–º 120 —Å–µ–∫—É–Ω–¥
  - —á–∏—Ç–∞–µ—Ç OLLAMA_CHAT_URL –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é http://127.0.0.1:4004/api/chat)
  - —Ñ—É–Ω–∫—Ü–∏—è async call_llm(messages, model=None):
    - –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç messages –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (role/content) –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
    - –ø—Ä–∏ model != None –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ "model" –≤ –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä "llama3.2:1b" –∏–ª–∏ "llama3.2:3b")
    - –æ–∂–∏–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç {"reply": "...", "raw": {...}}
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å OpenAI:
      {
        "model": <–∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ "default">,
        "choices": [
          {"message": {"role": "assistant", "content": reply}}
        ],
        "raw": <–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–∏—Å–∞>
      }
    - –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç LLMError —Å —Ç–µ–∫—Å—Ç–æ–º –≤–∏–¥–∞ "ollama_chat_4b error <code>: <body>"

5. –û—Å–Ω–æ–≤–Ω–æ–π FastAPI-—Å–µ—Ä–≤–∏—Å (app/main.py)
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI, –∫–æ—Ç–æ—Ä–æ–µ –≤ –±—É–¥—É—â–µ–º —Å—Ç–∞–Ω–µ—Ç REST-—Å–µ—Ä–≤–∏—Å–æ–º RAG:
  - title="RAG Service", version="0.1.0"
- –î–æ–±–∞–≤–ª–µ–Ω —ç–Ω–¥–ø–æ–∏–Ω—Ç GET /api/v1/health:
  - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON:
    {
      "status": "ok",
      "environment": "dev",
      "chromadb": "unknown",
      "sqlite": "unknown",
      "llm_mode": "ollama_chat_4b",
      "uptime_seconds": 0
    }
  - –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π health-check –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î/ChromaDB.
- –î–æ–±–∞–≤–ª–µ–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç GET /api/v1/llm_test:
  - –≤—ã–∑—ã–≤–∞–µ—Ç async call_llm –∏–∑ llm_connectors/connector_dev.py
  - –ø–µ—Ä–µ–¥–∞—ë—Ç –æ–¥–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ "–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: —Ç–µ—Å—Ç."
  - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞—ë—Ç model="llama3.2:1b"
  - –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ resp["choices"][0]["message"]["content"]
  - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON:
    - {"ok": True, "answer": "<–æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏>"} –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
    - {"ok": False, "error": "<—Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏>"} –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ LLMError
- –≠–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/llm_test —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω —á–µ—Ä–µ–∑ curl:
  - –ø—Ä–∏ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö ollama –∏ ollama_chat_4b –≤–æ–∑–≤—Ä–∞—Ç:
    {"ok": true, "answer": "–¢–µ—Å—Ç - ..."} (–∂–∏–≤–æ–π –æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏),
    —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ä–∞–±–æ—á—É—é —Å–≤—è–∑–∫—É rag_advct ‚Üí ollama_chat_4b ‚Üí Ollama.

6. .env –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
- –¢–µ–∫—É—â–∏–π .env —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
  - OLLAMA_CHAT_URL=http://127.0.0.1:4004/api/chat
- –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è rag_advct —Å–µ–π—á–∞—Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã, –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã LLM –æ—Ç–∫–ª—é—á–µ–Ω—ã.

7. –ò—Ç–æ–≥–∏ —à–∞–≥–∞
- –ï—Å—Ç—å —Ä–∞–±–æ—á–∏–π –∫–∞—Ä–∫–∞—Å —Å–µ—Ä–≤–∏—Å–∞ rag_advct:
  - FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å health-—ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–º –∏ —Ç–µ—Å—Ç–æ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ LLM.
  - –ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É LLM-—Å–µ—Ä–≤–∏—Å—É ollama_chat_4b.
  - –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è system.yaml –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ TaskConfig, RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ —Ä–µ–∂–∏–º–æ–≤ prod/dev/test.
- –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–µ—Ç–∫–µ):
  - —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å config_loader –∏ –º–æ–¥–µ–ª—å TaskConfig –ø–æ –¢–ó,
  - –¥–æ–±–∞–≤–∏—Ç—å —ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/task/query, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏ –∏ –ª–æ–∫–∞–ª—å–Ω—É—é LLM —á–µ—Ä–µ–∑ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä.


[2026-02-05 13:02] –î–µ–º–æ-—ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/task/query –∏ –∫–æ–Ω—Ñ–∏–≥-–ª–æ–∞–¥–µ—Ä.

1. –ö–æ–Ω—Ñ–∏–≥-–ª–æ–∞–¥–µ—Ä
- –î–æ–±–∞–≤–ª–µ–Ω –º–æ–¥—É–ª—å app/core/config_loader.py –Ω–∞ –±–∞–∑–µ pathlib + yaml.safe_load.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä—É–µ–º—ã–π load_system_config –∏ —Ö–µ–ª–ø–µ—Ä—ã get_environment, get_llm_mode, get_llm_connector_path.
- –ö–æ–Ω—Ñ–∏–≥ —á–∏—Ç–∞–µ—Ç—Å—è –∏–∑ config/system.yaml, –≤–∞–ª–∏–¥–∞—Ü–∏—è environment (dev/test/prod).

2. API /api/v1/task/query
- –î–æ–±–∞–≤–ª–µ–Ω —Ä–æ—É—Ç–µ—Ä app/api/v1/routes.py —Å APIRouter(prefix="/api/v1").
- –û–ø–∏—Å–∞–Ω—ã –º–æ–¥–µ–ª–∏ TaskQueryRequest/TaskQueryResponse –Ω–∞ Pydantic v2 (pattern –¥–ª—è task_type).
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω POST /api/v1/task/query:
  - –±–ª–æ–∫–∏—Ä—É–µ—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ (task_type=corporate) –ø—Ä–∏ environment != prod;
  - –¥–ª—è demo-–∑–∞–¥–∞—á –≤ dev –≤—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é LLM —á–µ—Ä–µ–∑ call_llm —Å –ø—Ä–æ—Å—Ç—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º;
  - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON {ok, answer|error}.

3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å FastAPI
- –í app/main.py –ø–æ–¥–∫–ª—é—á–µ–Ω —Ä–æ—É—Ç–µ—Ä api_v1_router.
- –≠–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/health –∏—Å–ø–æ–ª—å–∑—É–µ—Ç get_environment –∏ get_llm_mode –∏–∑ –∫–æ–Ω—Ñ–∏–≥-–ª–æ–∞–¥–µ—Ä–∞.
- –ü—Ä–æ–≤–µ—Ä–µ–Ω—ã —Ä—É—á–∫–∏ —á–µ—Ä–µ–∑ curl:
  - /api/v1/health –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å ok –∏ environment=dev;
  - /api/v1/task/query —Å demo-–∑–∞–¥–∞—á–µ–π –æ—Ç–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏;
  - /api/v1/task/query —Å corporate-–∑–∞–¥–∞—á–µ–π –≤ dev –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 403 "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ prod".


[2026-02-05 13:05] TaskConfig –∏ —Ñ–∞–π–ª–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ –∑–∞–¥–∞—á.

1. –ú–æ–¥–µ–ª—å TaskConfig
- –î–æ–±–∞–≤–ª–µ–Ω –º–æ–¥—É–ª—å app/core/task_config.py.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ dataclass-–º–æ–¥–µ–ª—å TaskConfig —Å –ø–æ–ª—è–º–∏ task_id, name, description, task_type, technical_prompt.
- –î–æ–±–∞–≤–ª–µ–Ω load_task_config(task_id), –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç config/tasks/<task_id>.yaml –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç task_type (demo/corporate).

2. –ö–æ–Ω—Ñ–∏–≥ –∑–∞–¥–∞—á–∏ demo_hello
- –°–æ–∑–¥–∞–Ω –∫–∞—Ç–∞–ª–æ–≥ config/tasks.
- –î–æ–±–∞–≤–ª–µ–Ω —Ñ–∞–π–ª config/tasks/demo_hello.yaml —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º TaskConfig:
  - task_id=demo_hello, task_type=demo;
  - —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ name/description;
  - technical_prompt —Å –±–∏–∑–Ω–µ—Å-–æ–ø–∏—Å–∞–Ω–∏–µ–º –¥–µ–º–æ-—Å–∏—Å—Ç–µ–º—ã –∑–∞—è–≤–æ–∫.

3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ /api/v1/task/query
- –≠–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/task/query –ø–µ—Ä–µ–∫–ª—é—á—ë–Ω –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TaskConfig:
  - –≥—Ä—É–∑–∏—Ç –∫–æ–Ω—Ñ–∏–≥ –ø–æ task_id —á–µ—Ä–µ–∑ load_task_config;
  - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ request.task_type –∏ TaskConfig.task_type;
  - –±–ª–æ–∫–∏—Ä—É–µ—Ç corporate-–∑–∞–¥–∞—á–∏ –ø—Ä–∏ environment != prod;
  - –¥–ª—è demo-–∑–∞–¥–∞—á –≤ dev –≤—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é LLM —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –ø—Ä–æ–º–ø—Ç–æ–º –∏–∑ TaskConfig.
- –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ curl —Å task_id=demo_hello –≤–µ—Ä–Ω—É–ª–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –±–∏–∑–Ω–µ—Å-–æ—Ç–≤–µ—Ç –æ –¥–µ–º–æ-—Å–∏—Å—Ç–µ–º–µ –∑–∞—è–≤–æ–∫.

## FILE: README.md
# N3R7A2C7

**Version:** 0.3

## –û–ø–∏—Å–∞–Ω–∏–µ

–°–∏—Å—Ç–µ–º–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞—è–≤–æ–∫ - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞—è–≤–æ–∫.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞—è–≤–æ–∫
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
- –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
N3R7A2C7/
‚îú‚îÄ‚îÄ README.md              # –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ CHANGELOG.md           # –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îú‚îÄ‚îÄ PROJECT_PLAN.md        # –ü–ª–∞–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
‚îú‚îÄ‚îÄ logs/                  # –õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã
‚îú‚îÄ‚îÄ docs/                  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îî‚îÄ‚îÄ src/                   # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥
```

## –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- –°–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–µ—Ä—Å–∏–π: Git/GitHub
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: Markdown
- –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

## –°—Ç–∞—Ç—É—Å

üîß –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ - –≤–µ—Ä—Å–∏—è 0.3

---
*–ü—Ä–æ–µ–∫—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ. –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Å–º. –≤ PROJECT_PLAN.md*

## FILE: app/api/v1/__init__.py


## FILE: app/api/v1/routes.py
from typing import Optional

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from app.core.config_loader import get_environment
from app.core.task_config import load_task_config, TaskConfigError
from llm_connectors.connector_dev import call_llm, LLMError


router = APIRouter(prefix="/api/v1", tags=["tasks"])


class TaskQueryRequest(BaseModel):
    task_id: str = Field(..., description="–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–¥–∞—á–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä demo_hello")
    task_type: str = Field(
        "demo",
        description="–¢–∏–ø –∑–∞–¥–∞—á–∏: demo –∏–ª–∏ corporate (–¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å TaskConfig)",
        pattern="^(demo|corporate)$",
    )
    query: str = Field(..., description="–¢–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")


class TaskQueryResponse(BaseModel):
    ok: bool
    answer: Optional[str] = None
    error: Optional[str] = None


@router.post("/task/query", response_model=TaskQueryResponse)
async def task_query(request: TaskQueryRequest) -> TaskQueryResponse:
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –ø–æ –¢–ó:
    - –≥—Ä—É–∑–∏—Ç TaskConfig –ø–æ task_id;
    - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å task_type –∏ environment;
    - –¥–ª—è demo-–∑–∞–¥–∞—á –≤ dev/test –≤—ã–∑—ã–≤–∞–µ—Ç LLM —Å —Ç–µ—Ö.–ø—Ä–æ–º–ø—Ç–æ–º –∏–∑ TaskConfig;
    - –¥–ª—è corporate –≤ dev/test –±–ª–æ–∫–∏—Ä—É–µ—Ç.
    """
    env = get_environment()

    try:
        task_cfg = load_task_config(request.task_id)
    except TaskConfigError as e:
        raise HTTPException(status_code=404, detail=str(e))

    if task_cfg.task_type != request.task_type:
        raise HTTPException(
            status_code=400,
            detail=f"Request task_type={request.task_type} does not match TaskConfig.task_type={task_cfg.task_type}",
        )

    if task_cfg.task_type == "corporate" and env != "prod":
        raise HTTPException(
            status_code=403,
            detail="–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ prod",
        )

    if task_cfg.task_type == "demo":
        system_prompt = task_cfg.technical_prompt or (
            "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –¥–µ–º–æ-RAG —Å–∏—Å—Ç–µ–º—ã. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": request.query},
        ]
        try:
            resp = await call_llm(messages, model="llama3.2:1b")
            answer = resp["choices"][0]["message"]["content"]
            return TaskQueryResponse(ok=True, answer=answer)
        except LLMError as e:
            return TaskQueryResponse(ok=False, error=str(e))

    raise HTTPException(status_code=400, detail="Unsupported task_type")

## FILE: app/core/__init__.py


## FILE: app/core/config_loader.py
from pathlib import Path
from functools import lru_cache
from typing import Any, Dict

import yaml


class SystemConfigError(Exception):
    pass


@lru_cache(maxsize=1)
def load_system_config() -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç config/system.yaml –æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ –∂–∏–∑–Ω—å –ø—Ä–æ—Ü–µ—Å—Å–∞.
    """
    config_path = Path(__file__).parent.parent.parent / "config" / "system.yaml"
    if not config_path.is_file():
        raise SystemConfigError(f"Config file not found: {config_path}")

    try:
        data = yaml.safe_load(config_path.read_text(encoding="utf-8"))
    except Exception as e:
        raise SystemConfigError(f"Failed to read system config: {e}") from e

    if not isinstance(data, dict):
        raise SystemConfigError("System config must be a mapping at top level")

    return data


def get_environment() -> str:
    cfg = load_system_config()
    env = cfg.get("environment", "dev")
    if env not in {"dev", "test", "prod"}:
        raise SystemConfigError(f"Invalid environment in config: {env}")
    return env


def get_llm_mode() -> str:
    cfg = load_system_config()
    llm = cfg.get("llm", {}) or {}
    mode = llm.get("mode", "dev")
    return mode


def get_llm_connector_path() -> str:
    cfg = load_system_config()
    llm = cfg.get("llm", {}) or {}
    connector = llm.get("connector")
    if not connector:
        raise SystemConfigError("llm.connector is not set in system config")
    return connector

## FILE: app/core/llm_client.py


## FILE: app/core/task_config.py
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict

import yaml


class TaskConfigError(Exception):
    pass


@dataclass
class TaskConfig:
    task_id: str
    name: str
    description: str
    task_type: str  # "demo" | "corporate"
    technical_prompt: str

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TaskConfig":
        try:
            task_id = data["task_id"]
            name = data.get("name", task_id)
            description = data.get("description", "")
            task_type = data.get("task_type", "demo")
            technical_prompt = data.get("technical_prompt", "")
        except KeyError as e:
            raise TaskConfigError(f"Missing required field in TaskConfig: {e}") from e

        if task_type not in {"demo", "corporate"}:
            raise TaskConfigError(f"Invalid task_type in TaskConfig: {task_type}")

        return cls(
            task_id=task_id,
            name=name,
            description=description,
            task_type=task_type,
            technical_prompt=technical_prompt,
        )


def load_task_config(task_id: str) -> TaskConfig:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç config/tasks/<task_id>.yaml –∏ –ø–∞—Ä—Å–∏—Ç –≤ TaskConfig.
    –°–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–ª—å–Ω–æ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π –∏–∑ –¢–ó.
    """
    tasks_dir = Path(__file__).parent.parent.parent / "config" / "tasks"
    cfg_path = tasks_dir / f"{task_id}.yaml"

    if not cfg_path.is_file():
        raise TaskConfigError(f"Task config not found for task_id={task_id}: {cfg_path}")

    try:
        data = yaml.safe_load(cfg_path.read_text(encoding="utf-8"))
    except Exception as e:
        raise TaskConfigError(f"Failed to read TaskConfig YAML: {e}") from e

    if not isinstance(data, dict):
        raise TaskConfigError("TaskConfig YAML must be a mapping at top level")

    return TaskConfig.from_dict(data)

## FILE: app/main.py
from fastapi import FastAPI
from fastapi.responses import JSONResponse

from llm_connectors.connector_dev import call_llm, LLMError
from app.api.v1.routes import router as api_v1_router
from app.core.config_loader import get_environment, get_llm_mode


app = FastAPI(title="RAG Service", version="0.1.0")

app.include_router(api_v1_router)


@app.get("/api/v1/health")
async def health():
    env = get_environment()
    llm_mode = get_llm_mode()
    return JSONResponse(
        {
            "status": "ok",
            "environment": env,
            "chromadb": "unknown",
            "sqlite": "unknown",
            "llm_mode": llm_mode or "unknown",
            "uptime_seconds": 0,
        }
    )


@app.get("/api/v1/llm_test")
async def llm_test():
    try:
        resp = await call_llm(
            [{"role": "user", "content": "–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: —Ç–µ—Å—Ç."}],
            model="llama3.2:1b",
        )
        content = resp["choices"][0]["message"]["content"]
        return {"ok": True, "answer": content}
    except LLMError as e:
        return {"ok": False, "error": str(e)}

## FILE: code_collector.py
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.resolve()
SNAPSHOT_FILE = PROJECT_ROOT / "all_code.txt"

INCLUDE_EXTS = {".py", ".yaml", ".yml", ".txt", ".md"}
EXCLUDE_FILES = {"all_code.txt", ".env"}


def should_include(path: Path) -> bool:
    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å–∫—Ä—ã—Ç—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (–Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å ".")
    for part in path.parts:
        if part.startswith("."):
            return False
    if path.name in EXCLUDE_FILES:
        return False
    if path.suffix not in INCLUDE_EXTS:
        return False
    return True


def collect_files():
    files = []
    seen = set()
    for p in PROJECT_ROOT.rglob("*"):
        if not p.is_file():
            continue
        rel = p.relative_to(PROJECT_ROOT)
        if rel in seen:
            continue
        if should_include(p):
            seen.add(rel)
            files.append(p)
    return sorted(files, key=lambda p: str(p))


def build_snapshot():
    files = collect_files()
    lines = []
    lines.append("# SNAPSHOT OF PROJECT FILES (TEXT)\n")
    lines.append("# Each file is separated by a marker line: ## FILE: <relative_path>\n\n")

    for path in files:
        rel_path = path.relative_to(PROJECT_ROOT)
        lines.append(f"## FILE: {rel_path}\n")
        try:
            text = path.read_text(encoding="utf-8")
        except Exception as e:
            lines.append(f"# ERROR READING FILE: {e}\n\n")
            continue
        lines.append(text)
        if not text.endswith("\n"):
            lines.append("\n")
        lines.append("\n")

    SNAPSHOT_FILE.write_text("".join(lines), encoding="utf-8")
    print(f"[code_collector] Snapshot written to {SNAPSHOT_FILE} ({len(files)} files).")


def main():
    print("code_collector.py ‚Äî —Å–Ω–∏–º–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞ –≤ all_code.txt")
    print("1) –û–±–Ω–æ–≤–∏—Ç—å all_code.txt (–≤—Å–µ —Ñ–∞–π–ª—ã –∫—Ä–æ–º–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞–ø–æ–∫)")
    choice = input("–í—ã–±–æ—Ä (1): ").strip() or "1"
    if choice == "1":
        build_snapshot()
    else:
        print("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≤—ã–±–æ—Ä, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—é.")


if __name__ == "__main__":
    main()

## FILE: config/system.yaml
environment: dev  # dev | test | prod

paths:
  data_root: "./data"
  chromadb_dir: "./data/chromadb"
  sqlite_path: "./data/sqlite/tables.db"
  logs_path: "./data/logs/app.log"

llm:
  mode: dev        # dev -> OpenRouter, prod/test –∑–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
  connector: "llm_connectors.connector_dev:call_llm"

## FILE: config/tasks/demo_hello.yaml
task_id: demo_hello
name: "Demo: –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞"
description: "–ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ-–∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ RAG-—Å–µ—Ä–≤–∏—Å–∞ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å LLM."
task_type: demo

technical_prompt: |
  –¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–∏—Å—Ç–µ–º—ã RAG, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞—è–≤–∫–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.
  –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –ø–æ –¥–µ–ª—É.
  –°–µ–π—á–∞—Å —É –Ω–∞—Å –¥–µ–º–æ-—Ä–µ–∂–∏–º: –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ç–∞–±–ª–∏—Ü –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–∏,
  —á—Ç–æ —ç—Ç–æ –∑–∞ –¥–µ–º–æ-—Å–∏—Å—Ç–µ–º–∞ –∑–∞—è–≤–æ–∫ –∏ —á—Ç–æ –æ–Ω–∞ –≤ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ –±—É–¥–µ—Ç —É–º–µ—Ç—å
  –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∏–∑–Ω–µ—Å–∞ (–±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π).

## FILE: docker/docker-compose.dev.yml


## FILE: llm_connectors/connector_dev.py
import os
from typing import List, Dict, Any

import httpx


OLLAMA_CHAT_URL = os.getenv(
    "OLLAMA_CHAT_URL",
    "http://127.0.0.1:4004/api/chat",
)


class LLMError(Exception):
    pass


async def call_llm(
    messages: List[Dict[str, Any]],
    model: str | None = None,
) -> Dict[str, Any]:
    """
    –î–µ–≤-–∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Å–µ—Ä–≤–∏—Å—É ollama_chat_4b.
    messages: —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (role/content).
    model: "llama3.2:1b" –∏–ª–∏ "llama3.2:3b" (–µ—Å–ª–∏ None ‚Äî –¥–µ—Ñ–æ–ª—Ç –≤ —Å–µ—Ä–≤–∏—Å–µ).
    """
    payload: Dict[str, Any] = {"messages": messages}
    if model:
        payload["model"] = model

    async with httpx.AsyncClient(timeout=120.0) as client:
        r = await client.post(OLLAMA_CHAT_URL, json=payload)
        if r.status_code != 200:
            raise LLMError(f"ollama_chat_4b error {r.status_code}: {r.text}")
        data = r.json()
        # –æ–∂–∏–¥–∞–µ–º {"reply": "...", "raw": {...}}
        reply = data.get("reply", "")
        return {
            "model": model or "default",
            "choices": [
                {"message": {"role": "assistant", "content": reply}}
            ],
            "raw": data.get("raw"),
        }

## FILE: tests/test_api.py


