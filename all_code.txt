# SNAPSHOT OF PROJECT FILES (TEXT)
# Each file is separated by a marker line: ## FILE: <relative_path>

## FILE: DEV_LOG.txt
[2026-02-05 11:59] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Ä–∫–∞—Å–∞ –ø—Ä–æ–µ–∫—Ç–∞ rag_advct –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ ollama_chat_4b.

1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
- –°–æ–∑–¥–∞–Ω –±–∞–∑–æ–≤—ã–π –∫–∞—Ä–∫–∞—Å –∫–∞—Ç–∞–ª–æ–≥–∞ rag_advct –ø–æ –¢–ó RAG-—Å–∏—Å—Ç–µ–º—ã:
  - app/ (–∏—Å—Ö–æ–¥–Ω–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞)
  - app/api/, app/core/, app/ingestion/, app/postprocessing/, app/bot/, app/cli/
  - config/ (system.yaml + –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è tasks/)
  - data/ (data/.gitkeep, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–¥ chromadb/sqlite/logs)
  - docker/ (Dockerfile, docker-compose.dev.yml ‚Äì –ø–æ–∫–∞ –ø—É—Å—Ç—ã–µ –∑–∞–≥–ª—É—à–∫–∏)
  - llm_connectors/ (–º–æ–¥—É–ª–∏ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LLM)
  - tests/ (test_api.py ‚Äì –∑–∞–≥–æ—Ç–æ–≤–∫–∞)
- –í .gitignore –¥–æ–±–∞–≤–ª–µ–Ω—ã:
  - .env
  - –∫–∞—Ç–∞–ª–æ–≥ data/ (–∫—Ä–æ–º–µ data/.gitkeep)
  - *.log, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ *.md.tmp, *.docx
  - all_code.py, DEV_LOG.txt, PROJECT_PLAN.txt
  —á—Ç–æ–±—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ –±—ã–ª–æ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–æ–≤ –∏ —Å–Ω–∞–ø—à–æ—Ç–æ–≤.

2. –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –°–æ–∑–¥–∞–Ω–æ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ .venv –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞.
- –í .venv —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:
  - fastapi[standard]
  - uvicorn
  - requests
  - httpx
- –í—Å–µ –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∫–æ–º–∞–Ω–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —á–µ—Ä–µ–∑:
  - source .venv/bin/activate

3. –°–∏—Å—Ç–µ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª config/system.yaml —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:
  - environment: dev
  - paths:
      data_root: ./data
      chromadb_dir: ./data/chromadb
      sqlite_path: ./data/sqlite/tables.db
      logs_path: ./data/logs/app.log
  - llm:
      mode: dev
      connector: "llm_connectors.connector_dev:call_llm"
- –≠—Ç–æ –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥-–ª–æ–∞–¥–µ—Ä–∞ –∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ (dev/test/prod) —Å—Ç—Ä–æ–≥–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥, –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ –¢–ó.

4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ ollama_chat_4b
- –ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö API (DeepSeek/OpenRouter) –≤ dev –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ –∏ –∑–∞–≤—è–∑–∫–∏ –Ω–∞ –±–∏–ª–ª–∏–Ω–≥.
- –í –∫–∞—á–µ—Å—Ç–≤–µ dev-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω—ã–π —Å—Ç–µ–∫:
  - –¥–≤–∏–∂–æ–∫ Ollama —Å –º–æ–¥–µ–ª—è–º–∏ llama3.2:1b –∏ llama3.2:3b
  - –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å ollama_chat_4b (FastAPI + uvicorn) —Å HTTP API:
    - POST http://127.0.0.1:4004/api/chat
    - —Ç–µ–ª–æ: {"messages": [...], "model": "llama3.2:1b"|"llama3.2:3b" (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)}
    - –æ—Ç–≤–µ—Ç: {"reply": "...", "raw": {... –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç Ollama ...}}
- –í rag_advct —Å–æ–∑–¥–∞–Ω –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä llm_connectors/connector_dev.py:
  - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç httpx.AsyncClient —Å —Ç–∞–π–º–∞—É—Ç–æ–º 120 —Å–µ–∫—É–Ω–¥
  - —á–∏—Ç–∞–µ—Ç OLLAMA_CHAT_URL –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é http://127.0.0.1:4004/api/chat)
  - —Ñ—É–Ω–∫—Ü–∏—è async call_llm(messages, model=None):
    - –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç messages –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (role/content) –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
    - –ø—Ä–∏ model != None –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ "model" –≤ –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä "llama3.2:1b" –∏–ª–∏ "llama3.2:3b")
    - –æ–∂–∏–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç {"reply": "...", "raw": {...}}
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å OpenAI:
      {
        "model": <–∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ "default">,
        "choices": [
          {"message": {"role": "assistant", "content": reply}}
        ],
        "raw": <–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–∏—Å–∞>
      }
    - –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç LLMError —Å —Ç–µ–∫—Å—Ç–æ–º –≤–∏–¥–∞ "ollama_chat_4b error <code>: <body>"

5. –û—Å–Ω–æ–≤–Ω–æ–π FastAPI-—Å–µ—Ä–≤–∏—Å (app/main.py)
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI, –∫–æ—Ç–æ—Ä–æ–µ –≤ –±—É–¥—É—â–µ–º —Å—Ç–∞–Ω–µ—Ç REST-—Å–µ—Ä–≤–∏—Å–æ–º RAG:
  - title="RAG Service", version="0.1.0"
- –î–æ–±–∞–≤–ª–µ–Ω —ç–Ω–¥–ø–æ–∏–Ω—Ç GET /api/v1/health:
  - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON:
    {
      "status": "ok",
      "environment": "dev",
      "chromadb": "unknown",
      "sqlite": "unknown",
      "llm_mode": "ollama_chat_4b",
      "uptime_seconds": 0
    }
  - –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π health-check –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î/ChromaDB.
- –î–æ–±–∞–≤–ª–µ–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç GET /api/v1/llm_test:
  - –≤—ã–∑—ã–≤–∞–µ—Ç async call_llm –∏–∑ llm_connectors/connector_dev.py
  - –ø–µ—Ä–µ–¥–∞—ë—Ç –æ–¥–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ "–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: —Ç–µ—Å—Ç."
  - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞—ë—Ç model="llama3.2:1b"
  - –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ resp["choices"][0]["message"]["content"]
  - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON:
    - {"ok": True, "answer": "<–æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏>"} –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
    - {"ok": False, "error": "<—Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏>"} –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ LLMError
- –≠–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/llm_test —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω —á–µ—Ä–µ–∑ curl:
  - –ø—Ä–∏ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö ollama –∏ ollama_chat_4b –≤–æ–∑–≤—Ä–∞—Ç:
    {"ok": true, "answer": "–¢–µ—Å—Ç - ..."} (–∂–∏–≤–æ–π –æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏),
    —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Ä–∞–±–æ—á—É—é —Å–≤—è–∑–∫—É rag_advct ‚Üí ollama_chat_4b ‚Üí Ollama.

6. .env –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
- –¢–µ–∫—É—â–∏–π .env —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
  - OLLAMA_CHAT_URL=http://127.0.0.1:4004/api/chat
- –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è rag_advct —Å–µ–π—á–∞—Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã, –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã LLM –æ—Ç–∫–ª—é—á–µ–Ω—ã.

7. –ò—Ç–æ–≥–∏ —à–∞–≥–∞
- –ï—Å—Ç—å —Ä–∞–±–æ—á–∏–π –∫–∞—Ä–∫–∞—Å —Å–µ—Ä–≤–∏—Å–∞ rag_advct:
  - FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å health-—ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–º –∏ —Ç–µ—Å—Ç–æ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ LLM.
  - –ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É LLM-—Å–µ—Ä–≤–∏—Å—É ollama_chat_4b.
  - –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è system.yaml –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ TaskConfig, RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ —Ä–µ–∂–∏–º–æ–≤ prod/dev/test.
- –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–µ—Ç–∫–µ):
  - —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å config_loader –∏ –º–æ–¥–µ–ª—å TaskConfig –ø–æ –¢–ó,
  - –¥–æ–±–∞–≤–∏—Ç—å —ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/task/query, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏ –∏ –ª–æ–∫–∞–ª—å–Ω—É—é LLM —á–µ—Ä–µ–∑ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä.

## FILE: README.md
# N3R7A2C7

**Version:** 0.3

## –û–ø–∏—Å–∞–Ω–∏–µ

–°–∏—Å—Ç–µ–º–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞—è–≤–æ–∫ - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞—è–≤–æ–∫.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞—è–≤–æ–∫
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
- –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
N3R7A2C7/
‚îú‚îÄ‚îÄ README.md              # –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ CHANGELOG.md           # –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îú‚îÄ‚îÄ PROJECT_PLAN.md        # –ü–ª–∞–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
‚îú‚îÄ‚îÄ logs/                  # –õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã
‚îú‚îÄ‚îÄ docs/                  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îî‚îÄ‚îÄ src/                   # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥
```

## –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- –°–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–µ—Ä—Å–∏–π: Git/GitHub
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: Markdown
- –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

## –°—Ç–∞—Ç—É—Å

üîß –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ - –≤–µ—Ä—Å–∏—è 0.3

---
*–ü—Ä–æ–µ–∫—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ. –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Å–º. –≤ PROJECT_PLAN.md*

## FILE: app/api/v1/__init__.py


## FILE: app/api/v1/routes.py


## FILE: app/core/__init__.py


## FILE: app/core/config_loader.py


## FILE: app/core/llm_client.py


## FILE: app/main.py
from fastapi import FastAPI
from fastapi.responses import JSONResponse

from llm_connectors.connector_dev import call_llm, LLMError

app = FastAPI(title="RAG Service", version="0.1.0")


@app.get("/api/v1/health")
async def health():
    return JSONResponse(
        {
            "status": "ok",
            "environment": "dev",
            "chromadb": "unknown",
            "sqlite": "unknown",
            "llm_mode": "ollama_chat_4b",
            "uptime_seconds": 0,
        }
    )


@app.get("/api/v1/llm_test")
async def llm_test():
    try:
        resp = await call_llm(
            [{"role": "user", "content": "–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: —Ç–µ—Å—Ç."}],
            model="llama3.2:1b",
        )
        content = resp["choices"][0]["message"]["content"]
        return {"ok": True, "answer": content}
    except LLMError as e:
        return {"ok": False, "error": str(e)}

## FILE: code_collector.py
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.resolve()
SNAPSHOT_FILE = PROJECT_ROOT / "all_code.txt"

INCLUDE_EXTS = {".py", ".yaml", ".yml", ".txt", ".md"}
EXCLUDE_FILES = {"all_code.txt", ".env"}


def should_include(path: Path) -> bool:
    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å–∫—Ä—ã—Ç—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (–Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å ".")
    for part in path.parts:
        if part.startswith("."):
            return False
    if path.name in EXCLUDE_FILES:
        return False
    if path.suffix not in INCLUDE_EXTS:
        return False
    return True


def collect_files():
    files = []
    seen = set()
    for p in PROJECT_ROOT.rglob("*"):
        if not p.is_file():
            continue
        rel = p.relative_to(PROJECT_ROOT)
        if rel in seen:
            continue
        if should_include(p):
            seen.add(rel)
            files.append(p)
    return sorted(files, key=lambda p: str(p))


def build_snapshot():
    files = collect_files()
    lines = []
    lines.append("# SNAPSHOT OF PROJECT FILES (TEXT)\n")
    lines.append("# Each file is separated by a marker line: ## FILE: <relative_path>\n\n")

    for path in files:
        rel_path = path.relative_to(PROJECT_ROOT)
        lines.append(f"## FILE: {rel_path}\n")
        try:
            text = path.read_text(encoding="utf-8")
        except Exception as e:
            lines.append(f"# ERROR READING FILE: {e}\n\n")
            continue
        lines.append(text)
        if not text.endswith("\n"):
            lines.append("\n")
        lines.append("\n")

    SNAPSHOT_FILE.write_text("".join(lines), encoding="utf-8")
    print(f"[code_collector] Snapshot written to {SNAPSHOT_FILE} ({len(files)} files).")


def main():
    print("code_collector.py ‚Äî —Å–Ω–∏–º–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞ –≤ all_code.txt")
    print("1) –û–±–Ω–æ–≤–∏—Ç—å all_code.txt (–≤—Å–µ —Ñ–∞–π–ª—ã –∫—Ä–æ–º–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞–ø–æ–∫)")
    choice = input("–í—ã–±–æ—Ä (1): ").strip() or "1"
    if choice == "1":
        build_snapshot()
    else:
        print("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≤—ã–±–æ—Ä, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—é.")


if __name__ == "__main__":
    main()

## FILE: config/system.yaml
environment: dev  # dev | test | prod

paths:
  data_root: "./data"
  chromadb_dir: "./data/chromadb"
  sqlite_path: "./data/sqlite/tables.db"
  logs_path: "./data/logs/app.log"

llm:
  mode: dev        # dev -> OpenRouter, prod/test –∑–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
  connector: "llm_connectors.connector_dev:call_llm"

## FILE: docker/docker-compose.dev.yml


## FILE: llm_connectors/connector_dev.py
import os
from typing import List, Dict, Any

import httpx


OLLAMA_CHAT_URL = os.getenv(
    "OLLAMA_CHAT_URL",
    "http://127.0.0.1:4004/api/chat",
)


class LLMError(Exception):
    pass


async def call_llm(
    messages: List[Dict[str, Any]],
    model: str | None = None,
) -> Dict[str, Any]:
    """
    –î–µ–≤-–∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Å–µ—Ä–≤–∏—Å—É ollama_chat_4b.
    messages: —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (role/content).
    model: "llama3.2:1b" –∏–ª–∏ "llama3.2:3b" (–µ—Å–ª–∏ None ‚Äî –¥–µ—Ñ–æ–ª—Ç –≤ —Å–µ—Ä–≤–∏—Å–µ).
    """
    payload: Dict[str, Any] = {"messages": messages}
    if model:
        payload["model"] = model

    async with httpx.AsyncClient(timeout=120.0) as client:
        r = await client.post(OLLAMA_CHAT_URL, json=payload)
        if r.status_code != 200:
            raise LLMError(f"ollama_chat_4b error {r.status_code}: {r.text}")
        data = r.json()
        # –æ–∂–∏–¥–∞–µ–º {"reply": "...", "raw": {...}}
        reply = data.get("reply", "")
        return {
            "model": model or "default",
            "choices": [
                {"message": {"role": "assistant", "content": reply}}
            ],
            "raw": data.get("raw"),
        }

## FILE: tests/test_api.py


